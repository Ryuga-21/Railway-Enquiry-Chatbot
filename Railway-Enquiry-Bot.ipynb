{"cells":[{"cell_type":"code","execution_count":26,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-18T09:01:07.709406Z","iopub.status.busy":"2023-07-18T09:01:07.708994Z","iopub.status.idle":"2023-07-18T09:01:07.720674Z","shell.execute_reply":"2023-07-18T09:01:07.719294Z","shell.execute_reply.started":"2023-07-18T09:01:07.709369Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/train-train2/intents.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:07.723269Z","iopub.status.busy":"2023-07-18T09:01:07.722802Z","iopub.status.idle":"2023-07-18T09:01:07.739789Z","shell.execute_reply":"2023-07-18T09:01:07.738369Z","shell.execute_reply.started":"2023-07-18T09:01:07.723171Z"},"trusted":true},"outputs":[],"source":["import nltk\n","#It contains text processing libraries for tokenization, \n","#parsing, classification, stemming, tagging and semantic reasoning.\n","from nltk.stem.lancaster import LancasterStemmer\n","stemmer = LancasterStemmer()\n","import curses\n","import string\n","import numpy"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:07.742536Z","iopub.status.busy":"2023-07-18T09:01:07.741229Z","iopub.status.idle":"2023-07-18T09:01:18.897995Z","shell.execute_reply":"2023-07-18T09:01:18.896582Z","shell.execute_reply.started":"2023-07-18T09:01:07.742493Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tflearn in /opt/conda/lib/python3.10/site-packages (0.5.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tflearn) (1.23.5)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tflearn) (1.16.0)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tflearn) (9.5.0)\n"]}],"source":["# !pip install tflearn"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.901328Z","iopub.status.busy":"2023-07-18T09:01:18.900910Z","iopub.status.idle":"2023-07-18T09:01:18.908769Z","shell.execute_reply":"2023-07-18T09:01:18.907645Z","shell.execute_reply.started":"2023-07-18T09:01:18.901272Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tflearn\n","\n","#TFlearn is a modular and transparent deep learning library built on top of Tensorflow. \n","#It was designed to provide a higher-level API to TensorFlow \n","#in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it.\n","import tensorflow.compat.v1 as tf\n","import random\n","tf.logging.set_verbosity(tf.logging.ERROR)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.910305Z","iopub.status.busy":"2023-07-18T09:01:18.909966Z","iopub.status.idle":"2023-07-18T09:01:18.926968Z","shell.execute_reply":"2023-07-18T09:01:18.925987Z","shell.execute_reply.started":"2023-07-18T09:01:18.910274Z"},"trusted":true},"outputs":[],"source":["import json\n","#open the exact location   of CORPUS \n","with open('/kaggle/input/train-train2/intents.json') as jd:\n","    intents = json.load(jd)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.928723Z","iopub.status.busy":"2023-07-18T09:01:18.928293Z","iopub.status.idle":"2023-07-18T09:01:18.938246Z","shell.execute_reply":"2023-07-18T09:01:18.937267Z","shell.execute_reply.started":"2023-07-18T09:01:18.928681Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["nltk.download('punkt')\n","#Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences,\n","#by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n","#It must be trained on a large collection of plaintext in the target language before it can be used.\n","#Create words, classes and documents\n","words = []\n","classes = []\n","documents = []\n","ignore_words = ['?','!']"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.939659Z","iopub.status.busy":"2023-07-18T09:01:18.939292Z","iopub.status.idle":"2023-07-18T09:01:18.961623Z","shell.execute_reply":"2023-07-18T09:01:18.960175Z","shell.execute_reply.started":"2023-07-18T09:01:18.939626Z"},"trusted":true},"outputs":[],"source":["# loop through each sentence in our intents patterns\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']: # PATTERN IS USER QUESTION\n","      \n","        # tokenize each word in the sentence\n","        \n","        w = nltk.word_tokenize(pattern)\n","        # add to our words list\n","        words.extend(w)\n","        # add to documents in our corpus        # add to our classes list\n","        if intent['tag'] not in classes:\n","\n","            documents.append((w, intent['tag']))\n","            classes.append(intent['tag'])\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.965160Z","iopub.status.busy":"2023-07-18T09:01:18.964508Z","iopub.status.idle":"2023-07-18T09:01:18.981574Z","shell.execute_reply":"2023-07-18T09:01:18.980162Z","shell.execute_reply.started":"2023-07-18T09:01:18.965125Z"},"trusted":true},"outputs":[],"source":["# stem and lower each word and remove duplicates\n","words = sorted(list(set([stemmer.stem(w.lower()) for w in words if w not in ignore_words])))\n","\n","# remove duplicates\n","classes = sorted(list(set(classes)))\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.983723Z","iopub.status.busy":"2023-07-18T09:01:18.983018Z","iopub.status.idle":"2023-07-18T09:01:18.997252Z","shell.execute_reply":"2023-07-18T09:01:18.996413Z","shell.execute_reply.started":"2023-07-18T09:01:18.983679Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["12 documents\n","12 classes ['book ticket', 'cancel', 'end', 'greeting', 'hours', 'opentoday', 'payment accept', 'pnr ', 'pnr_no ', 'refund ', 'schedule', 'thanks']\n","31 unique stemmed words [\"'s\", 'acceiv', 'ar', 'book', 'cancel', 'check', 'do', 'goodby', 'hi', 'hour', 'how', 'is', 'mod', 'my', 'no', 'of', 'op', 'pay', 'pnr', 'refund', 'schedule', 'stat', 'thank', 'the', 'ticket', 'to', 'today', 'train', 'what', 'when', 'you']\n"]}],"source":["#Getting to know the documents,classes and stemmed words\n","print (len(documents), \"documents\")\n","print (len(classes), \"classes\", classes)\n","print (len(words), \"unique stemmed words\", words)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:18.998567Z","iopub.status.busy":"2023-07-18T09:01:18.998189Z","iopub.status.idle":"2023-07-18T09:01:19.013154Z","shell.execute_reply":"2023-07-18T09:01:19.011918Z","shell.execute_reply.started":"2023-07-18T09:01:18.998535Z"},"trusted":true},"outputs":[],"source":["training = []\n","output = []\n","# create an empty array for our output\n","output_empty = [0] * len(classes)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:19.014791Z","iopub.status.busy":"2023-07-18T09:01:19.014405Z","iopub.status.idle":"2023-07-18T09:01:19.030600Z","shell.execute_reply":"2023-07-18T09:01:19.029399Z","shell.execute_reply.started":"2023-07-18T09:01:19.014758Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n","[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n","[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n","[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n","[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n"]}],"source":["# training set, bag of words for each sentence\n","for doc in documents:\n","    # initialize our bag of words\n","    bag = []    #bag of words which is a representaion of text that describes the occurence of wordss\n","    #with in a doccument\n","    # list of tokenized words for the pattern\n","    pattern_words = doc[0]\n","    # stem each word\n","    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n","    # create our bag of words array\n","    for w in words: #if the word appears in the pattern put 1 else 0 \n","        bag.append(1) if w in pattern_words else bag.append(0)\n","    print(bag)\n","    # output is a '0' for each tag and '1' for current tag\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:19.032511Z","iopub.status.busy":"2023-07-18T09:01:19.032132Z","iopub.status.idle":"2023-07-18T09:01:19.050435Z","shell.execute_reply":"2023-07-18T09:01:19.049163Z","shell.execute_reply.started":"2023-07-18T09:01:19.032478Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[list([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n","  list([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])]\n"," [list([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","  list([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"," [list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0])\n","  list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])]\n"," [list([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n","  list([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"," [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n","  list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])]\n"," [list([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n","  list([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])]\n"," [list([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1])\n","  list([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])]\n"," [list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n","  list([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])]\n"," [list([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n","  list([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"," [list([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n","  list([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])]\n"," [list([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","  list([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])]\n"," [list([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])\n","  list([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])]]\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_32/502661313.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  training = np.array(training)\n"]}],"source":["# shuffle our features and turn into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","print(training)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:19.052641Z","iopub.status.busy":"2023-07-18T09:01:19.052166Z","iopub.status.idle":"2023-07-18T09:01:19.062226Z","shell.execute_reply":"2023-07-18T09:01:19.060928Z","shell.execute_reply.started":"2023-07-18T09:01:19.052607Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]] [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]\n"]}],"source":["# create train and test lists\n","train_x = list(training[:,0])\n","train_y = list(training[:,1])\n","print(train_x,train_y)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:19.064103Z","iopub.status.busy":"2023-07-18T09:01:19.063771Z","iopub.status.idle":"2023-07-18T09:01:33.432432Z","shell.execute_reply":"2023-07-18T09:01:33.431294Z","shell.execute_reply.started":"2023-07-18T09:01:19.064064Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Step: 2999  | total loss: \u001b[1m\u001b[32m1.39557\u001b[0m\u001b[0m | time: 0.003s\n","| Adam | epoch: 1500 | loss: 1.39557 - acc: 0.9181 -- iter: 08/12\n","Training Step: 3000  | total loss: \u001b[1m\u001b[32m1.26069\u001b[0m\u001b[0m | time: 0.007s\n","| Adam | epoch: 1500 | loss: 1.26069 - acc: 0.9263 -- iter: 12/12\n","--\n"]}],"source":["# reset underlying graph data\n","tf.reset_default_graph()\n","# Build neural network\n","net = tflearn.input_data(shape=[None, len(train_x[0])])\n","net = tflearn.fully_connected(net, 12)\n","net = tflearn.fully_connected(net, 12)\n","net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n","net = tflearn.regression(net)\n","\n","# Define model and setup tensorboard\n","model = tflearn.DNN(net, tensorboard_dir='tflearn_logs') \n","# Start training (apply gradient descent algorithm)\n","model.fit(train_x, train_y, n_epoch=1500, batch_size=8, show_metric=True)#n_epoch is the number of times network sees the data\n","model.save('model.tflearn')"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:33.434079Z","iopub.status.busy":"2023-07-18T09:01:33.433764Z","iopub.status.idle":"2023-07-18T09:01:33.441576Z","shell.execute_reply":"2023-07-18T09:01:33.440387Z","shell.execute_reply.started":"2023-07-18T09:01:33.434052Z"},"trusted":true},"outputs":[],"source":["import pickle\n","pickle.dump({'words':words, 'classes':classes,'train_x':train_x,'train_y':train_y},open( \"training_data\", \"wb\" ))\n","\n","\n","\n","# restore all of our data structures\n","import pickle\n","data = pickle.load( open( \"training_data\", \"rb\" ) )\n","words = data['words']\n","classes = data['classes']\n","train_x = data['train_x']\n","train_y = data['train_y']"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:33.443647Z","iopub.status.busy":"2023-07-18T09:01:33.443169Z","iopub.status.idle":"2023-07-18T09:01:33.454757Z","shell.execute_reply":"2023-07-18T09:01:33.453839Z","shell.execute_reply.started":"2023-07-18T09:01:33.443603Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# train_x"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:01:33.456278Z","iopub.status.busy":"2023-07-18T09:01:33.455832Z","iopub.status.idle":"2023-07-18T09:01:33.586702Z","shell.execute_reply":"2023-07-18T09:01:33.585547Z","shell.execute_reply.started":"2023-07-18T09:01:33.456208Z"},"trusted":true},"outputs":[],"source":["# import our chat-bot intents file\n","# import json\n","# with open('intents.json') as jd:\n","#     intents = json.load(jd)\n","    \n","# load our saved model\n","model.load('./model.tflearn')\n","\n","def clean_up_sentence(sentence):\n","    # tokenize the pattern\n","    sentence_words = nltk.word_tokenize(sentence)\n","    # stem each word\n","    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","def bow(sentence, words, show_details=False):\n","    # tokenize the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","    # bag of words\n","    bag = [0]*len(words)  \n","    for s in sentence_words:\n","        for i,w in enumerate(words):\n","            if w == s: \n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","\n","    return(np.array(bag))"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:10:28.513816Z","iopub.status.busy":"2023-07-18T09:10:28.513440Z","iopub.status.idle":"2023-07-18T09:11:25.934495Z","shell.execute_reply":"2023-07-18T09:11:25.933489Z","shell.execute_reply.started":"2023-07-18T09:10:28.513786Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BOT: I am  your Personal Digital Assistant. What can I do for you!\n"]},{"name":"stdout","output_type":"stream","text":["You :  hi\n"]},{"name":"stdout","output_type":"stream","text":["BOT : Hi there, how can I help?\n"]},{"name":"stdout","output_type":"stream","text":["You :  when are you open today\n"]},{"name":"stdout","output_type":"stream","text":["BOT : We're 24*7 available\n"]},{"name":"stdout","output_type":"stream","text":["You :  how can i book tickets\n"]},{"name":"stdout","output_type":"stream","text":["BOT : go to our website and click buy option\n"]},{"name":"stdout","output_type":"stream","text":["You :  what are the payment methods\n"]},{"name":"stdout","output_type":"stream","text":["BOT : We accept VISA, Mastercard and AMEX , most major credit cards,Net banking\n"]},{"name":"stdout","output_type":"stream","text":["You :  Thank You\n"]},{"name":"stdout","output_type":"stream","text":["BOT : Any time!\n"]},{"name":"stdout","output_type":"stream","text":["You :  quit\n"]}],"source":["def cchat():\n","    print(\"BOT: I am  your Personal Digital Assistant. What can I do for you!\")\n","    while True:\n","        # inp=input(\"YOU : \")\n","        inp = input(\"You : \")\n","        if inp.lower()==\"quit\":\n","            break\n","        results = model.predict([bow(inp, words)])[0]\n","        results_index=numpy.argmax(results)\n","        tag=classes[results_index]\n","        if(results[results_index]>0.65):\n","            \n","            for tg in intents['intents']:\n","                if tg['tag']==tag:\n","                    response=tg['responses']\n","\n","            print(\"BOT : \" +random.choice(response))\n","        else:\n","            print(\"BOT :  I did not understand you! Try again\")\n","cchat()"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:12:28.213880Z","iopub.status.busy":"2023-07-18T09:12:28.213424Z","iopub.status.idle":"2023-07-18T09:12:39.741886Z","shell.execute_reply":"2023-07-18T09:12:39.740410Z","shell.execute_reply.started":"2023-07-18T09:12:28.213837Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting xlsxwriter\n","  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.1.2\n"]}],"source":["# !pip install xlsxwriter "]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:12:50.793251Z","iopub.status.busy":"2023-07-18T09:12:50.792780Z","iopub.status.idle":"2023-07-18T09:13:13.002711Z","shell.execute_reply":"2023-07-18T09:13:13.001742Z","shell.execute_reply.started":"2023-07-18T09:12:50.793209Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your FEEDBACK:  It was a good experience using the bot. Thank You!!\n"]}],"source":["import xlsxwriter \n","  \n","# Workbook() takes one, non-optional, argument  \n","# which is the filename that we want to create. \n","workbook = xlsxwriter.Workbook('feedback.xlsx') \n","  \n","# The workbook object is then used to add new  \n","# worksheet via the add_worksheet() method. \n","worksheet = workbook.add_worksheet() \n","  \n","# Use the worksheet object to write \n","# data via the write() method. \n","worksheet.write('A1', input(\"Enter your FEEDBACK: \" )) \n","\n","  \n","# Finally, close the Excel file \n","# via the close() method. \n","workbook.close() "]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:13.973346Z","iopub.status.busy":"2023-07-18T09:14:13.972924Z","iopub.status.idle":"2023-07-18T09:14:25.748622Z","shell.execute_reply":"2023-07-18T09:14:25.747043Z","shell.execute_reply.started":"2023-07-18T09:14:13.973289Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2023.5.7)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}],"source":["!pip install vaderSentiment"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:25.751700Z","iopub.status.busy":"2023-07-18T09:14:25.751060Z","iopub.status.idle":"2023-07-18T09:14:25.776981Z","shell.execute_reply":"2023-07-18T09:14:25.775726Z","shell.execute_reply.started":"2023-07-18T09:14:25.751656Z"},"trusted":true},"outputs":[],"source":["\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","analyser = SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:33.973003Z","iopub.status.busy":"2023-07-18T09:14:33.972598Z","iopub.status.idle":"2023-07-18T09:14:33.977998Z","shell.execute_reply":"2023-07-18T09:14:33.977055Z","shell.execute_reply.started":"2023-07-18T09:14:33.972970Z"},"trusted":true},"outputs":[],"source":["file=r'/kaggle/working/feedback.xlsx'"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:49.548009Z","iopub.status.busy":"2023-07-18T09:14:49.547628Z","iopub.status.idle":"2023-07-18T09:14:49.956235Z","shell.execute_reply":"2023-07-18T09:14:49.955313Z","shell.execute_reply.started":"2023-07-18T09:14:49.547980Z"},"trusted":true},"outputs":[],"source":["xl=pd.ExcelFile(file)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:50.974242Z","iopub.status.busy":"2023-07-18T09:14:50.972819Z","iopub.status.idle":"2023-07-18T09:14:50.989830Z","shell.execute_reply":"2023-07-18T09:14:50.988560Z","shell.execute_reply.started":"2023-07-18T09:14:50.974199Z"},"trusted":true},"outputs":[],"source":["dfs=xl.parse(xl.sheet_names[0])"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:14:57.283014Z","iopub.status.busy":"2023-07-18T09:14:57.282619Z","iopub.status.idle":"2023-07-18T09:14:57.302553Z","shell.execute_reply":"2023-07-18T09:14:57.301461Z","shell.execute_reply.started":"2023-07-18T09:14:57.282984Z"},"trusted":true},"outputs":[],"source":["sid=SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:15:03.772561Z","iopub.status.busy":"2023-07-18T09:15:03.772139Z","iopub.status.idle":"2023-07-18T09:15:03.778998Z","shell.execute_reply":"2023-07-18T09:15:03.777908Z","shell.execute_reply.started":"2023-07-18T09:15:03.772527Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["neg 0.0\n","neu 0.572\n","pos 0.428\n","compound 0.717\n"]}],"source":["for data in dfs:\n","    ss=sid.polarity_scores(data)\n","    for k in ss:\n","        print(k,ss[k])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"118d2d70b9837ec74e99cd8e271bd7c1e24309015268c38b027840ab45e80ec0"}}},"nbformat":4,"nbformat_minor":4}
